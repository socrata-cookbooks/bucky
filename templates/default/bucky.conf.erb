# Standard debug and log level
debug = False
log_level = "INFO"

# Whether to print the entire stack trace for errors encountered
# when loading the config file
full_trace = False

# Basic metricsd conifguration
<% if node['bucky']['metricsd_enabled'] %>
metricsd_ip = <%= node['bucky']['metricsd_ip'] %>
metricsd_port = <%= node['bucky']['metricsd_port'] %>
metricsd_enabled = <%= node['bucky']['metricsd_enabled'] %>

# The default interval between flushes of metric data to Graphite
metricsd_default_interval = 10.0

# You can specify the frequency of flushes to Graphite based on
# the metric name used for each metric. These are specified as
# regular expressions. An entry in this list should be a 3-tuple
# that is: (regexp, frequency, priority)
#
# The regexp is applied with the match method. Frequency should be
# in seconds. Priority is used to break ties when a metric name
# matches more than one handler. (The largest priority wins)
metricsd_handlers = []
<% end %>

# Basic collectd configuration
<% if node['bucky']['collectd_enabled'] %>
collectd_ip = <%= node['bucky']['collectd_ip'] %>
collectd_port = <%= node['bucky']['collectd_port'] %>
collectd_enabled = <%= node['bucky']['collectd_enabled'] %>

# A list of file names for collectd types.db
# files.
collectd_types = []

# A mapping of plugin names to converter callables. These are
# explained in more detail in the README.
collectd_converters = {}

# Whether to load converters from entry points. The entry point
# used to define converters is 'bucky.collectd.converters'.
collectd_use_entry_points = True

# If a collectd metric is received with a value of type counter when
# our types.db define it as derive, or vice versa, don't raise an
# exception and assume the server's types.db is correct.
# Types counter and derive are very similar. Also, it's common
# for different versions/installations of collectd in 'clients'
# to have a bit different definitions for the same metrics
# (counter/derive conflict).
collectd_counter_eq_derive = False

# CollectD server can also run using multiple worker subprocesses.
# Incoming packets are routed to workers based on source IP.
collectd_workers = 1

# Cryptographic settings for collectd. Security level 1 requires
# signed packets, level 2 requires encrypted communication.
# Auth file should contain lines in the form 'user: password'
collectd_security_level = 0
collectd_auth_file = None
<% end %>

# Basic statsd configuration
<% if node['bucky']['statsd_enabled'] %>
statsd_ip = <%= node['bucky']['statsd_ip'] %>
statsd_port = <%= node['bucky']['statsd_port'] %>
statsd_enabled = <%= node['bucky']['statsd_enabled'] %>

# Don't keep sending stats that aren't being reported
statsd_delete_idlestats = True

# How often stats should be flushed to Graphite.
statsd_flush_time = 10.0

# If the legacy namespace is enabled, the statsd backend uses the
# default prefixes except for counters, which are stored directly
# in stats.NAME for the rate and stats_counts.NAME for the
# absolute count.  If legacy names are disabled, the prefixes are
# configurable, and counters are stored under
# stats.counters.{rate,count} by default.  Any prefix can be set
# to None to skip it.
statsd_legacy_namespace = False
statsd_global_prefix = None
statsd_prefix_counter = None
statsd_prefix_timer = None
statsd_prefix_gauge = None
<% end %>

# If the Graphite connection fails these numbers define how it
# will reconnect. The max reconnects applies each time a
# disconnect is encountered and the reconnect delay is the time
# in seconds between connection attempts. Setting max reconnects
# to a negative number removes the limit. The backoff factor
# determines how much the reconnect delay will be multiplied with
# each reconnect round. It can be limited with a maximum after which
# the delay will not be multiplied anymore.
graphite_max_reconnects = 3
graphite_reconnect_delay = 5
graphite_backoff_factor = 1.5
graphite_backoff_max = 60

# Configuration for sending metrics to Graphite via the pickle
# interface. Be sure to edit graphite_port to match the settings
# on your Graphite cache/relay.
graphite_pickle_enabled = False
graphite_pickle_buffer_size = 500

# Bucky provides these settings to allow the system wide
# configuration of how metric names are processed before
# sending to Graphite.
#
# Prefix and postfix allow to tag all values with some value.
name_prefix = <%= node['bucky']['name_prefix'] %>
name_postfix = <%= node['bucky']['name_postfix'] %>

# The replacement character is used to munge any '.' characters
# in name components because it is special to Graphite. Setting
# this to None will prevent this step.
name_replace_char = None

# Optionally strip duplicates in path components. For instance
# a.a.b.c.c.b would be rewritten as a.b.c.b
name_strip_duplicates = True

# Bucky reverses hostname components to improve the locality
# of metric values in Graphite. For instance, "node.company.tld"
# would be rewritten as "tld.company.node". This setting allows
# for the specification of hostname components that should
# be stripped from hostnames. For instance, if "company.tld"
# were specified, the previous example would end up as "node".
name_host_trim = <%= node['bucky']['name_host_trim_array'] %>

# processor is a callable that takes a (host, name, val, time)
# tuple as input and is expected to return a tuple of the same
# structure to forward the sample to the clients, or None to
# drop it. processor_drop_on_error specifies if the sample is
# dropped or forwarded to clients in case an exception is
# raised by the processor callable.

import socket
hostname = socket.gethostname()

def rewrite_consul(host, name, val, ts):
    wo_pre = '.'.join((_ for _ in name.split('.')
        if _ != 'consul' and _ != hostname))
    name = '{}.consul.{}'.format(hostname, wo_pre)
    return host, name, val, ts


def manage_namespace(host, name, val, ts):
    if name.startswith('consul.'):
        return rewrite_consul(host, name, val, ts)
    else:
        return host, name, val, ts

processor = manage_namespace
processor_drop_on_error = False
